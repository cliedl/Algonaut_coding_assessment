{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ff82ebf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee80866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_papers():\n",
    "    \"\"\"Fetches papers from the arXiv API and returns them as a list of strings.\"\"\"\n",
    "\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=ti:llama&start=0&max_results=70'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    data = response.read().decode('utf-8')\n",
    "    root = ET.fromstring(data)\n",
    "\n",
    "    papers_list = []\n",
    "\n",
    "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
    "        title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
    "        summary = entry.find('{http://www.w3.org/2005/Atom}summary').text\n",
    "        paper_info = f\"Title: {title}\\nSummary: {summary}\\n\"\n",
    "        papers_list.append(paper_info)\n",
    "\n",
    "    return papers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0493090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TitleAbstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Lawyer LLaMA Technical Report\\nSummary:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Title: Label Supervised LLaMA Finetuning\\nSumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Title: LLAMA: Leveraging Learning to Automatic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Title: Challenges and opportunities integratin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Title: LLaMA: Open and Efficient Foundation La...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      TitleAbstract\n",
       "0           0  Title: Lawyer LLaMA Technical Report\\nSummary:...\n",
       "1           1  Title: Label Supervised LLaMA Finetuning\\nSumm...\n",
       "2           2  Title: LLAMA: Leveraging Learning to Automatic...\n",
       "3           3  Title: Challenges and opportunities integratin...\n",
       "4           4  Title: LLaMA: Open and Efficient Foundation La..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#papers_list = fetch_papers()\n",
    "#df = pd.DataFrame(papers_list, columns=['TitleAbstract'])\n",
    "df = pd.read_csv('df_papers_llama2.csv')\n",
    "#df.to_csv('df_papers_llama2.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d59f910-ba5a-4fab-afd1-571ad1a1c4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2058"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TitleAbstract'].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313eb46e-d5d1-4137-874b-8e55bd3a2a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1131"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def remove_linebreaks(input_string):\n",
    "    \"\"\"Removes linebreaks and tabs from string using regular expression\"\"\"\n",
    "    cleaned_string = re.sub(r'[\\n\\t]', ' ', input_string)\n",
    "    return cleaned_string\n",
    "    \n",
    "df['TitleAbstract'] = df['TitleAbstract'].apply(remove_linebreaks)\n",
    "\n",
    "import spacy\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Load the spaCy NLP model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract keywords (nouns and adjectives)\n",
    "    keywords = [token.text for token in doc if token.pos_ in [\"NOUN\", \"ADJ\"]]\n",
    "    keywords = \" \".join(keywords)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "\n",
    "df['KeyWords'] = df['TitleAbstract'].apply(extract_keywords)\n",
    "\n",
    "df['KeyWords'].apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c1e1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def get_bert_embedding(input_string):\n",
    "    \"\"\"tokenizes string and runs BERT to obtain embedding.\"\"\"\n",
    "\n",
    "    # Load pre-trained BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Tokenize input text\n",
    "    tokens = tokenizer(input_string, return_tensors='pt')\n",
    "\n",
    "    # Forward pass to obtain embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "\n",
    "    # Extract embeddings from the last hidden layer (CLS token)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "    return embeddings[0]\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    \"\"\"Calculates cosine similarity between two embeddings.\"\"\"\n",
    "\n",
    "    # Ensure both embeddings have the same shape\n",
    "    if embedding1.shape != embedding2.shape:\n",
    "        raise ValueError(\"Embeddings must have the same shape for cosine similarity calculation.\")\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = np.dot(embedding1, embedding2.T) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "    return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db61c6d5-10b1-4fbb-aba5-af0566031877",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Embedding'] = df['KeyWords'].apply(get_bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8f1c1569-a5ce-427a-a247-c9ed0f8a3f59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TitleAbstract</th>\n",
       "      <th>KeyWords</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: Lawyer LLaMA Technical Report Summary: ...</td>\n",
       "      <td>Title Large remarkable performance various tas...</td>\n",
       "      <td>[-0.21004462, 0.5068891, 0.39497513, 0.3047706...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: Label Supervised LLaMA Finetuning Summa...</td>\n",
       "      <td>Title recent success Large significant attenti...</td>\n",
       "      <td>[-0.62233067, 0.41749716, 0.07669323, 0.112751...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: LLAMA: Leveraging Learning to Automatic...</td>\n",
       "      <td>Title portfolio selection approaches remarkabl...</td>\n",
       "      <td>[-0.8397804, 0.3386485, 0.20808932, 0.18489337...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title: Challenges and opportunities integratin...</td>\n",
       "      <td>Title Challenges opportunities transport simul...</td>\n",
       "      <td>[-0.5284588, 0.3198053, 0.34352037, 0.10175034...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Title: LLaMA: Open and Efficient Foundation La...</td>\n",
       "      <td>Title collection foundation language models 7B...</td>\n",
       "      <td>[-0.48944178, -0.0042269714, -0.015640128, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       TitleAbstract  \\\n",
       "0  Title: Lawyer LLaMA Technical Report Summary: ...   \n",
       "1  Title: Label Supervised LLaMA Finetuning Summa...   \n",
       "2  Title: LLAMA: Leveraging Learning to Automatic...   \n",
       "3  Title: Challenges and opportunities integratin...   \n",
       "4  Title: LLaMA: Open and Efficient Foundation La...   \n",
       "\n",
       "                                            KeyWords  \\\n",
       "0  Title Large remarkable performance various tas...   \n",
       "1  Title recent success Large significant attenti...   \n",
       "2  Title portfolio selection approaches remarkabl...   \n",
       "3  Title Challenges opportunities transport simul...   \n",
       "4  Title collection foundation language models 7B...   \n",
       "\n",
       "                                           Embedding  \n",
       "0  [-0.21004462, 0.5068891, 0.39497513, 0.3047706...  \n",
       "1  [-0.62233067, 0.41749716, 0.07669323, 0.112751...  \n",
       "2  [-0.8397804, 0.3386485, 0.20808932, 0.18489337...  \n",
       "3  [-0.5284588, 0.3198053, 0.34352037, 0.10175034...  \n",
       "4  [-0.48944178, -0.0042269714, -0.015640128, 0.0...  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b657b6b3-bc59-4e6f-9991-acd16c273306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([12, 24, 8, 17, 54, 25, 26, 4, 47, 35], dtype='int64')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Context information is below.\\n---------------\\nTitle: HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge Summary:   Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo, a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge. Our proposed HuaTuo model is accessible at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.  \\nTitle: Tamil-Llama: A New Tamil Language Model Based on Llama 2 Summary:   Language modeling has witnessed remarkable advancements in recent years, with Large Language Models (LLMs) like ChatGPT setting unparalleled benchmarks in human-like text generation. However, a prevailing limitation is the underrepresentation of languages like Tamil in these cutting-edge models, leading to suboptimal performance in diverse linguistic contexts. This paper addresses this lacuna, enhancing the open-source LLaMA model with an addition of 16,000 Tamil tokens, aiming to achieve superior text generation and comprehension in the Tamil language. We strategically employ the LoRA methodology for efficient model training on a comprehensive Tamil corpus, ensuring computational feasibility and model robustness. Moreover, we introduce a Tamil-translated version of the Alpaca dataset and a subset of the OpenOrca dataset tailored for instruction fine-tuning. Our results showcase significant performance improvements in Tamil text generation, with potential implications for the broader landscape of LLMs in Indian languages. We further underscore our commitment to open research by making our models, datasets, and code publicly accessible, fostering further innovations in language modeling.  \\nTitle: Steering Llama 2 via Contrastive Activation Addition Summary:   We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying activations during their forward passes. CAA computes ``steering vectors'' by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using both multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, outperforms traditional methods like finetuning and few-shot prompting, and minimally reduces capabilities. Moreover, by employing various activation space interpretation methods, we gain deeper insights into CAA's mechanisms. CAA both accurately steers model outputs and also sheds light on how high-level concepts are represented in Large Language Models (LLMs).  \\nTitle: Llama 2: Open Foundation and Fine-Tuned Chat Models Summary:   In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.  \\nTitle: Baby Llama: knowledge distillation from an ensemble of teachers trained   on a small dataset with no performance penalty Summary:   We present our submission to the BabyLM challenge, whose goal was to improve the sample efficiency of language models. We trained an ensemble consisting of a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model, which exceeds in performance both of its teachers as well as a similar model trained without distillation. This suggests that distillation can not only retain the full performance of the teacher model when the latter is trained on a sufficiently small dataset; it can exceed it, and lead to significantly better performance than direct training.  \\nTitle: Beyond Surface: Probing LLaMA Across Scales and Layers Summary:   This paper presents an in-depth analysis of Large Language Models (LLMs), focusing on LLaMA, a prominent open-source foundational model in natural language processing. Instead of assessing LLaMA through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of LLaMA lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge.  \\nTitle: Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models Summary:   This paper presents CyberSecEval, a comprehensive benchmark developed to help bolster the cybersecurity of Large Language Models (LLMs) employed as coding assistants. As what we believe to be the most extensive unified cybersecurity safety benchmark to date, CyberSecEval provides a thorough evaluation of LLMs in two crucial security domains: their propensity to generate insecure code and their level of compliance when asked to assist in cyberattacks. Through a case study involving seven models from the Llama 2, Code Llama, and OpenAI GPT large language model families, CyberSecEval effectively pinpointed key cybersecurity risks. More importantly, it offered practical insights for refining these models. A significant observation from the study was the tendency of more advanced models to suggest insecure code, highlighting the critical need for integrating security considerations in the development of sophisticated LLMs. CyberSecEval, with its automated test case generation and evaluation pipeline covers a broad scope and equips LLM designers and researchers with a tool to broadly measure and enhance the cybersecurity safety properties of LLMs, contributing to the development of more secure AI systems.  \\nTitle: LLaMA: Open and Efficient Foundation Language Models Summary:   We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.  \\nTitle: Localizing Lying in Llama: Understanding Instructed Dishonesty on   True-False Questions Through Prompting, Probing, and Patching Summary:   Large language models (LLMs) demonstrate significant knowledge through their outputs, though it is often unclear whether false outputs are due to a lack of knowledge or dishonesty. In this paper, we investigate instructed dishonesty, wherein we explicitly prompt LLaMA-2-70b-chat to lie. We perform prompt engineering to find which prompts best induce lying behavior, and then use mechanistic interpretability approaches to localize where in the network this behavior occurs. Using linear probing and activation patching, we localize five layers that appear especially important for lying. We then find just 46 attention heads within these layers that enable us to causally intervene such that the lying model instead answers honestly. We show that these interventions work robustly across many prompts and dataset splits. Overall, our work contributes a greater understanding of dishonesty in LLMs so that we may hope to prevent it.  \\nTitle: Multi-Task Instruction Tuning of LLaMa for Specific Scenarios: A   Preliminary Study on Writing Assistance Summary:   Proprietary Large Language Models (LLMs), such as ChatGPT, have garnered significant attention due to their exceptional capabilities in handling a diverse range of tasks. Recent studies demonstrate that open-sourced smaller foundational models, such as 7B-size LLaMA, can also display remarkable proficiency in tackling diverse tasks when fine-tuned using instruction-driven data. In this work, we investigate a practical problem setting where the primary focus is on one or a few particular tasks rather than general-purpose instruction following, and explore whether LLMs can be beneficial and further improved for such targeted scenarios. We choose the writing-assistant scenario as the testbed, which includes seven writing tasks. We collect training data for these tasks, reframe them in an instruction-following format, and subsequently refine the LLM, specifically LLaMA, via instruction tuning. Experimental results show that fine-tuning LLaMA on writing instruction data significantly improves its ability on writing tasks. We also conduct more experiments and analyses to offer insights for future work on effectively fine-tuning LLaMA for specific scenarios. Finally, we initiate a discussion regarding the necessity of employing LLMs for only one targeted task, taking into account the efforts required for tuning and the resources consumed during deployment.  \\n---------------\\n'\\nGiven the context information and prior knowledge, answer the following query.\\n \\nQuery:\\xa0For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Name at least 5 domain-specific LLMs that have been created by fine-tuning Llama-2.\"\n",
    "question = \"For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?\"\n",
    "\n",
    "embedding_question = get_bert_embedding(question)\n",
    "similarities = df['Embedding'].apply(lambda embedding_paper:cosine_similarity(embedding_question, embedding_paper))\n",
    "index_match = similarities.sort_values(ascending=False).head(10).index\n",
    "print(index_match)\n",
    "\n",
    "\n",
    "relevant_papers = \"\\n\".join(df['TitleAbstract'][index_match].values)\n",
    "\n",
    "query = f\"\"\"Context information is below.\\n---------------\\nContext:\\n{relevant_papers}\\n---------------\\n'\n",
    "Given the context information and prior knowledge, answer the following query.\\n \n",
    "Query:Â {question}\"\"\"\n",
    "\n",
    "#query = 'CONTEXT: '+relevant_papers+' QUESTION:'+question \n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27aae6c9-9675-41ce-adf2-ecd6af45f956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From the given context, it is not explicitly mentioned which tasks Llama-2 has been used successfully for. However, based on the information provided, we can infer some potential areas of application where Llama-2 shows promise:\\n\\n1. Biomedical domain tasks: Llama-2 has been proposed to improve the performance of Large Language Models (LLMs) in biomedical domain tasks by fine-tuning it with generated Question-Answer (QA) instances. This approach resulted in more reliable medical knowledge in the model's responses.\\n\\n2. Tamil language tasks: Llama-2 has been enhanced with the addition of 16,000 Tamil tokens, aiming to achieve superior text generation and comprehension in the Tamil language. This suggests that Llama-2 can be successfully applied to tasks involving Tamil language processing.\\n\\n3. Dialogue-based tasks: Llama-2, specifically the Llama 2-Chat models, have been fine-tuned and optimized for dialogue use cases. These models outperform open-source chat\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=None)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "  ],\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "998b1b98-50d8-4572-9691-02e7a636d725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-2 has been used successfully for various natural language processing (NLP) tasks, including:\\n\\n1. Language Modeling: Llama-2 has been trained on a large corpus of text to generate coherent and contextually accurate language. It can be used for tasks like text generation, completion, and paraphrasing.\\n\\n2. Text Classification: Llama-2 has shown promising results in classifying text into predefined categories or labels. It can be used for sentiment analysis, spam detection, topic classification, etc.\\n\\n3. Named Entity Recognition (NER): Llama-2 can extract specific information from text by identifying and classifying named entities such as names, dates, organizations, locations, etc.\\n\\n4. Summarization: Llama-2 has been used for automatic text summarization, condensing large documents or articles into concise summaries while retaining the key points.\\n\\n5. Machine Translation: Llama-2 can be utilized for machine translation tasks, enabling the translation of text between different languages.\\n\\n'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=None)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "  ],\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c79b7-6e87-4b6a-8c69-3220db06bf38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
