{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98da5245-8ec5-4672-90dc-da62745db4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from model import get_embedding, cosine_similarity, get_relevant_papers, build_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52832484-0195-4157-8b1b-7afa20aef913",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_papers_llama2.csv\")\n",
    "Embeddings = np.load(\"Embeddings.npy\", allow_pickle=True)\n",
    "df[\"Embedding\"] = Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "466728ee-6ac9-4541-915c-b21fdef8ac7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\"For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?\",\n",
    "             \"Name at least 5 domain-specific LLMs that have been created by fine-tuning Llama-2.\",\n",
    "             \"What can you find out about the model structure of Llama-2 (required memory, required computing capacity, number of parameters, available quantizations)?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a900cab-d406-4886-9069-85afbe9fea2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "<class 'str'> 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\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: For which tasks has Llama-2 already been used successfully? What are promising areas of application for Llama-2?\n",
      "\n",
      "Based on the given context information, it is mentioned in the title \"Llama 2: Open Foundation and Fine-Tuned Chat Models\" that Llama 2 models have been optimized for dialogue use cases and outperformed open-source chat models on most benchmarks. This suggests that Llama-2 has been successfully used for chat-based tasks and conversational applications.\n",
      "\n",
      "Additionally, in the title \"ChatGPT, Llama, can you write my report? An experiment on assisted digital forensics reports written using (Local) Large Language Models,\" it is mentioned that LLMs like Llama have been explored for assisting in the forensic report writing process in the context of digital forensics investigations. This indicates another successful application area for Llama-2 in assisting with report generation.\n",
      "\n",
      "Therefore, Llama-2 has been used successfully for chat-based tasks and assisting with report writing in the context of digital forensics investigations. Promising areas for the application of Llama-2 may include conversational AI, chatbots, virtual assistants, customer support systems, and automated report generation in various domains.\n",
      "\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Question: Name at least 5 domain-specific LLMs that have been created by fine-tuning Llama-2.\n",
      "\n",
      "Based on the given context, here are five domain-specific LLMs that have been created by fine-tuning Llama-2:\n",
      "\n",
      "1. Llama 2-Chat: Large language models optimized for dialogue use cases.\n",
      "2. HuaTuo: A LLaMA-based model supervised-fine-tuned with generated QA instances in the biomedical domain.\n",
      "3. VinaLLaMA: An open-weight large language model for the Vietnamese language with an additional 800 billion trained tokens.\n",
      "4. LLaMA Russian Adaptation: An adaptation of LLaMA for the Russian language, addressing tokenization issues in non-English input.\n",
      "5. Code Llama: A family of large language models for code, including foundation models, Python specializations, and instruction-following models.\n",
      "\n",
      "Please note that these are just examples of domain-specific LLMs created by fine-tuning Llama-2, and there may be others as well.\n",
      "\n",
      "--------------------------------------------------------------\n",
      "\n",
      "Question: What can you find out about the model structure of Llama-2 (required memory, required computing capacity, number of parameters, available quantizations)?\n",
      "\n",
      "Based on the given context, there is no specific information available about the model structure of Llama-2, including its required memory, computing capacity, number of parameters, or available quantizations. The context primarily focuses on the performance, fine-tuning, biases, adaptations, and applications of various Llama models across different domains and languages.\n",
      "\n",
      "--------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Input api_key\n",
    "api_key = input(str)\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Input question\n",
    "#question = input(str)\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print()\n",
    "    \n",
    "    # Get relevant papers\n",
    "    paper_ids, relevant_papers = get_relevant_papers(question, df, num_papers=10)\n",
    "    # Construct prompt from question and relevant papers\n",
    "    prompt = build_prompt(question, relevant_papers)\n",
    "    \n",
    "    # Query GPT-3.5-turbo\n",
    "    response = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    )\n",
    "    \n",
    "    # Print response\n",
    "    print(response.choices[0].message.content)\n",
    "    print('\\n--------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd0d082-dff5-4637-b6d9-e69628d536c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
